In [[statistics]], '''expected mean squares (EMS)''' are the expected values of certain statistics arising in partitions of sums of squares in the [[analysis of variance]] (ANOVA). They can be used for ascertaining which statistic should appear in the denominator in an [[F-test]] for testing a [[null hypothesis]] that a particular effect is absent.

== Definition ==

When the total corrected sum of squares in an ANOVA is partitioned into several components, each attributed to the effect of a particular predictor variable, each of the sums of squares in that partition is a random variable that has an [[expected value]]. That expected value divided by the corresponding number of degrees of freedom is the expected [[Analysis_of_variance#Partitioning_of_the_sum_of_squares|mean square]] for that predictor variable.

== Example ==

The following example is from ''Longitudinal Data Analysis'' by Donald Hedeker and Robert D. Gibbons.<ref>  Donald Hedeker, Robert D. Gibbons. ''Longitudinal Data Analysis.'' Wiley Interscience. 2006. pp.&nbsp;21–24</ref>

Each of ''s'' treatments (one of which may be a placebo) is administered to a sample of (capital) ''N'' randomly chosen patients, on whom certain measurements <math display="inline"> Y_{hij} </math> are observed at each of (lower-case) ''n'' specified times, for <math display="inline"> h=1,\ldots,s, \quad i=1,\ldots,N_h </math> (thus the numbers of patients receiving different treatments may differ), and <math display="inline"> j=1,\ldots, n.</math> We assume the sets of patients receiving different treatments are disjoint, so patients are [[Restricted_randomization#Nested_random_effects|nested]] within treatments and not crossed with treatments. We have

: <math> Y_{hij} = \mu + \gamma_h + \tau_j + (\gamma\tau)_{hj} + \pi_{i(h)} + \varepsilon_{hij} </math>

where

: <math>
\begin{align}
\mu & = \text{grand mean}, & & \text{(fixed)} \\
\gamma_h & = \text{effect of treatment } h, & & \text{(fixed)} \\
\tau_j & = \text{effect of time } j, & & \text{(fixed)} \\
(\gamma\tau)_{hj} & = \text{interaction effect of treatment } h \text{ and time } j, & & \text{(fixed)} \\
\pi_{i(h)} & = \text{individual difference effect for patient } i \text{ nested within treatment } h, & & \text{(random)} \\
\varepsilon_{hij} & = \text{error for patient } i \text{ in treatment } h \text{ at time } j. & & \text{(random)} \\
\sigma_\pi^2 & = \text{variance of the random effect of patients nested within treatments,} \\
\sigma_\varepsilon & = \text{error variance.}
\end{align}
</math>

The total corrected sum of squares is

: <math> \sum_{hij} (Y_{hij} - \overline Y)^2 \quad\text{where } \overline Y = \frac 1 n \sum_{hij} Y_{hij}. </math>

The ANOVA table below partitions the sum of squares (where <math display="inline"> N = \sum_h N_h </math>):

: <math>
\begin{array}{|r|c|l|c|l|}
\hline
\begin{array}{c} \text{source of} \\ \text{variability} \end{array} & \begin{array}{c} \text{degrees of} \\ \text{freedom} \end{array} & \text{sum of squares} & \text{mean square} & \begin{array}{c} \text{expected} \\ \text{mean} \\ \text{square} \end{array} \\
\hline
\text{treatment} & s-1 & \text{SS}_\text{Tr} = n\sum_{h=1}^s N_h(\overline Y_{h\cdot\cdot} - \overline Y_{\cdot\cdot\cdot})^2 & \dfrac{\text{SS}_\text{Tr}}{s-1} & \sigma_\varepsilon^2 + n \sigma_\pi^2 + D_\text{Tr} \\[6pt]
\text{time} & n-1 & \text{SS}_\text{T} = N \sum_{j=1}^n (\overline Y_{\cdot\cdot j} - \overline Y_{\cdot\cdot\cdot})^2 & \dfrac{\text{SS}_\text{T}}{n-1} & \sigma_\varepsilon^2 + D_\text{T} \\[6pt]
\text{treatment} \times \text{time} & (s-1)(n-1) & \text{SS}_\text{Tr T} = \sum_{h=1}^s \sum_{j=1}^n N_h (\overline Y_{h\cdot j} - \overline Y_{h\cdot\cdot} - \overline Y_{\cdot\cdot j} + \overline Y_{\cdot\cdot\cdot})^2 & \dfrac{\text{SS}_\text{Tr T}}{(n-1)(s-1)} & \sigma_\varepsilon^2 + D_\text{Tr T} \\[6pt]
\begin{array}{c} \text{patients} \\ \text{within} \\ \text{treatments} \end{array} & N-s & \text{SS}_{\text{S}(\text{Tr})} = n\sum_{h=1}^s \sum_{i=1}^{N_h} (\overline Y_{hi\cdot} - \overline Y_{h\cdot\cdot})^2 & \dfrac{\text{SS}_{\text{S}(\text{Tr})}}{N-s} & \sigma_\varepsilon^2 + n\sigma_\pi^2 \\[6pt]
\text{error} & (N-s)(n-1) & \text{SS}_\text{E} = \sum_{h=1}^s \sum_{i=1}^{N_h} \sum_{j=1}^n (Y_{hij} - \overline Y_{h\cdot j} - \overline Y_{hi\cdot} + \overline Y_{h\cdot\cdot})^2 & \dfrac{\text{SS}_\text{E}}{(N-s)(n-1)} & \sigma_\varepsilon^2 \\ \hline
\end{array}
</math>

=== Use in F-tests ===

A null hypothesis of interest is that there is no difference between effects of different treatments—thus no difference among treatment means. This may be expressed by saying <math display="inline"> D_\text{Tr}=0, </math> (with the notation as used in the table above). Under this null hypothesis, the expected mean square for effects of treatments is <math display="inline"> \sigma_\varepsilon^2 + n \sigma_\pi^2. </math>

The numerator in the F-statistic for testing this hypothesis is the mean square due to differences among treatments, i.e.&nbsp;it is <math display="inline> \left. \text{SS}_\text{Tr} \right/(s-1). </math> The denominator, however, is '''not''' <math display="inline"> \left. \text{SS}_\text{E}\right/ \big( (N-s)(n-1) \big). </math> The reason is that the random variable below, although under the null hypothesis it has an [[F-distribution]], is not observable—it is not a statistic—because its value depends on the unobservable parameters <math display="inline"> \sigma_\pi^2 </math> and <math display="inline"> \sigma_\varepsilon^2. </math>

: <math> \frac{\left.\frac{\text{SS}_\text{Tr}}{\sigma_\varepsilon^2 + n\sigma_\pi^2} \right/(s-1)}{ \left. \frac{\text{SS}_\text{E}}{\sigma_\varepsilon^2} \right/ \big( (N-s)(n-1) \big)} \ne \frac{\text{SS}_\text{Tr}/(s-1)}{\text{SS}_\text{E}/\big((N-s)(n-1)\big)} </math>

Instead, one uses as the test statistic the following random variable that is not defined in terms of <math display=inline> \text{SS}_\text{E}</math>:

: <math> F = \frac{\left.\frac{\text{SS}_\text{Tr}}{\sigma_\varepsilon^2 + n\sigma_\pi^2} \right/(s-1)}{ \left. \frac{\text{SS}_{\text{S}(\text{Tr})}}{\sigma_\varepsilon^2+ n\sigma_\pi^2} \right/ (N-s)} = \frac{\left. \text{SS}_\text{Tr} \right/(s-1)}{ \left. \text{SS}_\text{S(Tr)} \right/ (N-s)} </math>

== Notes and references ==

{{reflist}}

[[Category:Statistics]]