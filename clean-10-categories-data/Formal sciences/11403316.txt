{{short description|Signal processing technique}}
{{pp-semi|small=yes}}
'''Compressed sensing''' (also known as '''compressive sensing''', '''compressive sampling''', or '''sparse sampling''') is a [[signal processing]] technique for efficiently acquiring and reconstructing a [[Signal (electronics)|signal]], by finding solutions to [[Underdetermined system|underdetermined linear systems]]. This is based on the principle that, through optimization, the sparsity of a signal can be exploited to recover it from far fewer samples than required by the [[Nyquist–Shannon sampling theorem]]. There are two conditions under which recovery is possible.<ref>[http://nuit-blanche.blogspot.com/2009/09/cs.html CS: Compressed Genotyping, DNA Sudoku – Harnessing high throughput sequencing for multiplexed specimen analysis].</ref> The first one is [[sparsity]], which requires the signal to be sparse in some domain. The second one is [[Coherence (signal processing)|incoherence]], which is applied through the isometric property, which is sufficient for sparse signals.<ref>{{cite journal | last1 = Donoho | first1 = David L. | s2cid = 8510060 | year = 2006 | title =  For most large underdetermined systems of linear equations the minimal 1-norm solution is also the sparsest solution| journal = Communications on Pure and Applied Mathematics | volume = 59 | issue = 6| pages = 797–829 | doi = 10.1002/cpa.20132 }}</ref><ref>M. Davenport, [http://www.brainshark.com/brainshark/brainshark.net/portal/title.aspx?pid=zCdz10BfTRz0z0 "The Fundamentals of Compressive Sensing"], SigView, April 12, 2013.</ref>

== Overview ==
A common goal of the engineering field of [[signal processing]] is to reconstruct a signal from a series of sampling measurements. In general, this task is impossible because there is no way to reconstruct a signal during the times that the signal is not measured. Nevertheless, with prior knowledge or assumptions about the signal, it turns out to be possible to perfectly reconstruct a signal from a series of measurements (acquiring this series of measurements is called [[Sampling (signal processing)|sampling]]). Over time, engineers have improved their understanding of which assumptions are practical and how they can be generalized.

An early breakthrough in signal processing was the [[Nyquist–Shannon sampling theorem]]. It states that if a [[Real number|real]] signal's highest frequency is less than half of the sampling rate, then the signal can be reconstructed perfectly by means of [[Whittaker–Shannon interpolation formula|sinc interpolation]]. The main idea is that with prior knowledge about constraints on the signal's frequencies, fewer samples are needed to reconstruct the signal.

Around 2004, [[Emmanuel Candès]], [[Justin Romberg]], [[Terence Tao]], and [[David Donoho]] proved that given knowledge about a signal's [[sparsity]], the signal may be reconstructed with even fewer samples than the sampling theorem requires.<ref>{{Cite journal|doi=10.1002/cpa.20124|url=http://www-stat.stanford.edu/~candes/papers/StableRecovery.pdf|title=Stable signal recovery from incomplete and inaccurate measurements|year=2006|last1=Candès|first1=Emmanuel J.|last2=Romberg|first2=Justin K.|last3=Tao|first3=Terence|journal=Communications on Pure and Applied Mathematics|volume=59|issue=8|pages=1207–1223|arxiv=math/0503066|bibcode=2005math......3066C|s2cid=119159284|access-date=2011-02-10|archive-url=https://web.archive.org/web/20120311192231/http://www-stat.stanford.edu/~candes/papers/StableRecovery.pdf|archive-date=2012-03-11|url-status=dead}}</ref><ref name=Donoho>{{Cite journal|doi=10.1109/TIT.2006.871582|title=Compressed sensing|year=2006|last1=Donoho|first1=D.L.|journal=IEEE Transactions on Information Theory|volume=52|issue=4|pages=1289–1306|s2cid=206737254}}</ref> This idea is the basis of compressed sensing.

==History==
Compressed sensing relies on [[Lp space|L1]] techniques, which several other scientific fields have used historically.<ref>[http://2.bp.blogspot.com/_0ZCyAOBrUtA/TTwqLEeLvdI/AAAAAAAAEXI/7S0_SnWoC0E/s1600/l1-minimization.JPG List of L1 regularization ideas] from Vivek Goyal, Alyson Fletcher, Sundeep Rangan, [http://www.math.uiuc.edu/%7Elaugesen/imaha10/goyal_talk.pdf The Optimistic Bayesian: Replica Method Analysis of Compressed Sensing]</ref> In statistics, the [[least squares]] method was complemented by the [[Lp norm|<math>L^1</math>-norm]], which was introduced by [[Pierre-Simon Laplace|Laplace]]. Following the introduction of [[linear programming]] and [[George Dantzig|Dantzig]]'s [[simplex algorithm]], the <math>L^1</math>-norm was used in [[computational statistics]]. In statistical theory, the <math>L^1</math>-norm was used by [[George W. Brown (academic)|George W. Brown]] and later writers on [[median-unbiased estimator]]s. It was used by Peter J. Huber and others working on [[robust statistics]]. The <math>L^1</math>-norm was also used in signal processing, for example, in the 1970s, when seismologists constructed images of reflective layers within the earth based on data that did not seem to satisfy the [[Nyquist–Shannon sampling theorem|Nyquist–Shannon criterion]].<ref>{{Cite journal |doi = 10.1511/2009.79.276 |title = The Best Bits |year = 2009 |last1 = Hayes |first1 = Brian |s2cid = 349102 |journal = American Scientist |volume = 97 |issue = 4 |pages = 276 }}</ref>  It was used in [[matching pursuit]] in 1993, the [[Lasso regression|LASSO estimator]] by [[Robert Tibshirani]] in 1996<ref>{{Cite journal |url = http://www-stat.stanford.edu/~tibs/lasso.html |first = Robert |last = Tibshirani |title = Regression shrinkage and selection via the lasso |journal = [[Journal of the Royal Statistical Society, Series B]] |volume = 58 |issue = 1 |pages = 267–288 }}</ref> and [[basis pursuit]] in 1998.<ref>"Atomic decomposition by basis pursuit", by Scott Shaobing Chen, David L. Donoho, Michael, A. Saunders. SIAM Journal on Scientific Computing</ref>  There were theoretical results describing when these algorithms recovered sparse solutions, but the required type and number of measurements were sub-optimal and subsequently greatly improved by compressed sensing.{{citation needed|date=May 2013}}

At first glance, compressed sensing might seem to violate [[Nyquist–Shannon sampling theorem|the sampling theorem]], because compressed sensing depends on the [[Sparse matrix|sparsity]] of the signal in question and not its highest frequency. This is a misconception, because the sampling theorem guarantees perfect reconstruction given sufficient, not necessary, conditions. A sampling method fundamentally different from classical fixed-rate sampling cannot "violate" the sampling theorem. Sparse signals with high frequency components can be highly under-sampled using compressed sensing compared to classical fixed-rate sampling.<ref>{{Cite journal |url = http://www-stat.stanford.edu/~candes/papers/ExactRecovery.pdf |title = Robust Uncertainty Principles: Exact Signal Reconstruction from Highly Incomplete Fourier Information |year = 2006 |last1 = Candès |first1 = Emmanuel J. |last2 = Romberg |first2 = Justin K. |last3 = Tao |first3 = Terence |journal = IEEE Trans. Inf. Theory |volume = 52 |issue = 8 |pages = 489–509 |doi=10.1109/tit.2005.862083|arxiv = math/0409186 |citeseerx = 10.1.1.122.4429 |s2cid = 7033413 }}</ref>

==Method==

===Underdetermined linear system===
An [[underdetermined system]] of linear equations has more unknowns than equations and generally has an infinite number of solutions. The figure below shows such an equation system <math> \mathbf{y}=D\mathbf{x} </math> where we want to find a solution for <math> \mathbf{x} </math>.

[[File:Underdetermined equation system.svg|200px|alt=Underdetermined linear equation system|Underdetermined linear equation system]]

In order to choose a solution to such a system, one must impose extra constraints or conditions (such as smoothness) as appropriate. In compressed sensing, one adds the constraint of sparsity, allowing only solutions which have a small number of nonzero coefficients. Not all underdetermined systems of linear equations have a sparse solution. However, if there is a unique sparse solution to the underdetermined system, then the compressed sensing framework allows the recovery of that solution.

===Solution / reconstruction method===
[[File:Orthogonal Matching Pursuit.gif|500px|thumb|right|Example of the retrieval of an unknown signal (gray line) from few measurements (black dots) using the knowledge that the signal is sparse in the Hermite polynomials basis (purple dots show the retrieved coefficients).]]
Compressed sensing takes advantage of the redundancy in many interesting signals—they are not pure noise. In particular, many signals are [[sparse matrix|sparse]], that is, they contain many coefficients close to or equal to zero, when represented in some domain.<ref>Candès, E.J., & Wakin, M.B., ''An Introduction To Compressive Sampling'', IEEE Signal Processing Magazine, V.21, March 2008 [http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4472240&isnumber=4472102]</ref> This is the same insight used in many forms of [[lossy compression]].

Compressed sensing typically starts with taking a weighted linear combination of samples also called compressive measurements in a [[Basis (linear algebra)|basis]] different from the basis in which the signal is known to be sparse. The results found by  [[Emmanuel Candès]], [[Justin Romberg]],  [[Terence Tao]] and  [[David Donoho]], showed that the number of these compressive measurements can be small and still contain nearly all the useful information. Therefore, the task of converting the image back into the intended domain involves solving an underdetermined [[matrix equation]] since the number of compressive measurements taken is smaller than the number of pixels in the full image. However, adding the constraint that the initial signal is sparse enables one to solve this underdetermined [[system of linear equations]].

The least-squares solution to such problems is to minimize the [[L2 norm|<math>L^2</math> norm]]—that is, minimize the amount of energy in the system. This is usually simple mathematically (involving only a [[matrix multiplication]] by the [[pseudo-inverse]] of the basis sampled in). However, this leads to poor results for many practical applications, for which the unknown coefficients have nonzero energy.

To enforce the sparsity constraint when solving for the underdetermined system of linear equations, one can minimize the number of nonzero components of the solution. The function counting the number of non-zero components of a vector was called the [[L0 norm|<math>L^0</math> "norm"]] by David Donoho{{refn|group=note|The quotation marks served two warnings.  First, the number-of-nonzeros <math>L^0</math>-"norm" is not a proper [[F-space|F-norm]], because it is not continuous in its scalar argument: ''nnzs''(α''x'') is constant as α approaches zero. Unfortunately, authors now neglect the quotation marks and [[abuse of terminology|abused terminology]]—clashing with the established use of the <math>L^0</math> norm for the space of measurable functions (equipped with an appropriate metric) or for the [[F-space|space]] of sequences with [[F-space|F–norm]] <math>(x_n) \mapsto \sum_n{2^{-n} x_n/(1+x_n)}</math>.<ref>Stefan Rolewicz. ''Metric Linear Spaces''.</ref>}}.

[[Emmanuel Candès|Candès]] et al. proved that for many problems it is probable that the [[L1 norm|<math>L^1</math> norm]] is equivalent to the [[L0 norm|<math>L^0</math> norm]], in a technical sense: This equivalence result allows one to solve the <math>L^1</math> problem, which is easier than the <math>L^0</math> problem. Finding the candidate with the smallest <math>L^1</math> norm can be expressed relatively easily as a [[linear program]], for which efficient solution methods already exist.<ref>[http://www.acm.caltech.edu/l1magic/ L1-MAGIC is a collection of MATLAB routines]</ref>  When measurements may contain a finite amount of noise, [[basis pursuit denoising]] is preferred over linear programming, since it preserves sparsity in the face of noise and can be solved faster than an exact linear program.

=== Total variation based CS reconstruction ===
{{see also|Total variation denoising}}
{{split section|Total variation reconstruction|date=May 2017}}<!-- reason=Longest section in current article. -->

==== Motivation and applications ====

===== Role of TV regularization =====
[[Total variation]] can be seen as a [[non-negative]] [[real number|real]]-valued [[functional (mathematics)|functional]] defined on the space of [[real number|real-valued]] [[function (mathematics)|function]]s (for the case of functions of one variable) or on the space of [[integrable function]]s (for the case of functions of several variables). For signals, especially, [[total variation]] refers to the integral of the absolute [[gradient]] of the signal. In signal and image reconstruction, it is applied as [[total variation regularization]] where the underlying principle is that signals with excessive details have high total variation and that removing these details, while retaining important information such as edges, would reduce the total variation of the signal and make the signal subject closer to the original signal in the problem.

For the purpose of signal and image reconstruction, <math>l1</math> minimization models are used. Other approaches also include the least-squares as has been discussed before in this article. These methods are extremely slow and return a not-so-perfect reconstruction of the signal. The current CS Regularization models attempt to address this problem by incorporating sparsity priors of the original image, one of which is the total variation (TV). Conventional TV approaches are designed to give piece-wise constant solutions. Some of these include (as discussed ahead) – constrained l1-minimization which uses an iterative scheme. This method, though fast, subsequently leads to over-smoothing of edges resulting in blurred image edges.<ref name = "EPTV" /> TV methods with iterative re-weighting have been implemented to reduce the influence of large gradient value magnitudes in the images. This has been used in [[Tomography|computed tomography]] (CT) reconstruction as a method known as edge-preserving total variation. However, as gradient magnitudes are used for estimation of relative penalty weights between the data fidelity and regularization terms, this method is not robust to noise and artifacts and accurate enough for CS image/signal reconstruction and, therefore, fails to preserve smaller structures.

Recent progress on this problem involves using an iteratively directional TV refinement for CS reconstruction.<ref name = "Orientation and directional refinement" /> This method would have 2 stages: the first stage would estimate and refine the initial orientation field – which is defined as a noisy point-wise initial estimate, through edge-detection, of the given image. In the second stage, the CS reconstruction model is presented by utilizing directional TV regularizer. More details about these TV-based approaches – iteratively reweighted l1 minimization, edge-preserving TV and iterative model using directional orientation field and TV- are provided below.

==== Existing approaches ====

=====Iteratively reweighted <math>l_{1}</math> minimization =====
[[File:IRLS.png|thumb|iteratively reweighted l1 minimization method for CS]]
In the CS reconstruction models using constrained <math>l_{1}</math> minimization,<ref name="Original source for IRLS">{{cite journal | last1 = Candes | first1 = E. J. | last2 = Wakin | first2 = M. B. | last3 = Boyd | first3 = S. P. | year = 2008 | title = Enhancing sparsity by reweighted l1 minimization | journal = J. Fourier Anal. Applicat | volume = 14 | issue = 5–6| pages = 877–905 | doi=10.1007/s00041-008-9045-x| arxiv = 0711.1612 | s2cid = 5879257 }}</ref> larger coefficients are penalized heavily in the <math>l_{1}</math> norm. It was proposed to have a weighted formulation of <math>l_{1}</math> minimization designed to more democratically penalize nonzero coefficients. An iterative algorithm is used for constructing the appropriate weights.<ref name="Iteration">Lange, K.: Optimization, Springer Texts in Statistics. Springer, New York (2004)</ref> Each iteration requires solving one <math>l_{1}</math> minimization problem by finding the local minimum of a concave penalty function that more closely resembles the <math>l_{0}</math> norm. An additional parameter, usually to avoid any sharp transitions in the penalty function curve, is introduced into the iterative equation to ensure stability and so that a zero estimate in one iteration does not necessarily lead to a zero estimate in the next iteration. The method essentially involves using the current solution for computing the weights to be used in the next iteration.

====== Advantages and disadvantages ======
Early iterations may find inaccurate sample estimates, however this method will down-sample these at a later stage to give more weight to the smaller non-zero signal estimates. One of the disadvantages is the need for defining a valid starting point as a global minimum might not be obtained every time due to the concavity of the function. Another disadvantage is that this method tends to uniformly penalize the image gradient irrespective of the underlying image structures. This causes over-smoothing of edges, especially those of low contrast regions, subsequently leading to loss of low contrast information. The advantages of this method include: reduction of the sampling rate for sparse signals; reconstruction of the image while being robust to the removal of noise and other artifacts; and use of very few iterations. This can also help in recovering images with sparse gradients.

In the figure shown below, '''P1''' refers to the first-step of the iterative reconstruction process, of the projection matrix '''P''' of the fan-beam geometry, which is constrained by the data fidelity term. This may contain noise and artifacts as no regularization is performed. The minimization of '''P1''' is solved through the conjugate gradient least squares method. '''P2''' refers to the second step of the iterative reconstruction process wherein it utilizes the edge-preserving total variation regularization term to remove noise and artifacts, and thus improve the quality of the reconstructed image/signal. The minimization of '''P2''' is done through a simple gradient descent method. Convergence is determined by testing, after each iteration, for image positivity, by checking if <math>f^{k-1} = 0</math> for the case when <math>f^{k-1} < 0</math> (Note that <math>f</math> refers to the different x-ray linear attenuation coefficients at different voxels of the patient image).

=====Edge-preserving total variation (TV) based compressed sensing=====
[[File:Edge preserving TV.png|thumb|Flow diagram figure for edge preserving total variation method for compressed sensing]]
This is an iterative CT reconstruction algorithm with edge-preserving TV regularization to reconstruct CT images from highly undersampled data obtained at low dose CT through low current levels (milliampere).  In order to reduce  the imaging dose, one of the approaches used is to reduce the number of x-ray projections acquired by the scanner detectors. However, this insufficient projection data which is used to reconstruct the CT image can cause streaking artifacts. Furthermore, using these insufficient projections in standard TV algorithms end up making the problem under-determined and thus leading to infinitely many possible solutions. In this method, an additional penalty weighted function is assigned to the original TV norm. This allows for easier detection of sharp discontinuities in intensity in the images and thereby adapt the weight to store the recovered edge information during the process of signal/image reconstruction. The parameter <math>\sigma</math> controls the amount of smoothing applied to the pixels at the edges to differentiate them from the non-edge pixels. The value of <math>\sigma</math> is changed adaptively based on the values of the histogram of the gradient magnitude so that a certain percentage of pixels have gradient values larger than <math>\sigma</math>. The edge-preserving total variation term, thus, becomes sparser and this speeds up the implementation. A two-step iteration process known as forward-backward splitting algorithm is used.<ref name = "Forward-Backward">{{cite journal | last1 = Combettes | first1 = P | last2 = Wajs | first2 = V | s2cid = 15064954 | year = 2005 | title = Signal recovery by proximal forward-backward splitting | journal = Multiscale Model Simul | volume = 4 | issue = 4| pages = 1168–200 | doi=10.1137/050626090}}</ref> The optimization problem is split into two sub-problems which are then solved with the conjugate gradient least squares method<ref name="CGLS">{{cite journal | last1 = Hestenes | first1 = M | last2 = Stiefel | first2 = E | year = 1952 | title = Methods of conjugate gradients for solving linear systems | journal =  Journal of Research of the National Bureau of Standards| volume = 49 | issue = 6| pages = 409–36 | doi=10.6028/jres.049.044| doi-access = free }}</ref> and the simple gradient descent method respectively. The method is stopped when the desired convergence has been achieved or if the maximum number of iterations is reached.<ref name ="EPTV">{{cite journal | last1 = Tian | first1 = Z. | last2 = Jia | first2 = X. | last3 = Yuan | first3 = K. | last4 = Pan | first4 = T. | last5 = Jiang | first5 = S. B. | year = 2011 | title = Low-dose CT reconstruction via edge preserving total variation regularization | journal = Phys Med Biol | volume = 56 | issue = 18| pages = 5949–5967 | doi=10.1088/0031-9155/56/18/011| pmid = 21860076 | pmc = 4026331 | arxiv = 1009.2288 | bibcode = 2011PMB....56.5949T }}</ref>

===== Advantages and disadvantages =====
Some of the disadvantages of this method are the absence of smaller structures in the reconstructed image and degradation of image resolution. This edge preserving TV algorithm, however, requires fewer iterations than the conventional TV algorithm.<ref name ="EPTV" /> Analyzing the horizontal and vertical intensity profiles of the reconstructed images, it can be seen that there are sharp jumps at edge points and negligible, minor fluctuation at non-edge points. Thus, this method leads to low relative error and higher correlation as compared to the TV method. It also effectively suppresses and removes any form of image noise and image artifacts such as streaking.

=====Iterative model using a directional orientation field and directional total variation=====
To prevent over-smoothing of edges and texture details and to obtain a reconstructed CS image which is accurate and robust to noise and artifacts, this method is used. First, an initial estimate of the noisy point-wise orientation field of the image <math>I</math>, <math>\hat{d}</math>, is obtained. This noisy orientation field is defined so that it can be refined at a later stage to reduce the noise influences in orientation field estimation. A coarse orientation field estimation is then introduced based on structure tensor which is formulated as:<ref name="Structure tensor">{{cite journal | last1 = Brox | first1 = T. | last2 = Weickert | first2 = J. | last3 = Burgeth | first3 = B. | last4 = Mrázek | first4 = P. | year = 2006 | title = Nonlinear structure tensors | journal = Image Vis. Comput | volume = 24 | issue = 1| pages = 41–55 | doi=10.1016/j.imavis.2005.09.010| citeseerx = 10.1.1.170.6085 }}</ref> <math> J_\rho(\nabla I_{\sigma}) = G_\rho * (\nabla I_{\sigma} \otimes \nabla I_{\sigma}) = \begin{pmatrix}J_{11} & J_{12}\\J_{12} & J_{22}\end{pmatrix}</math>. Here, <math> J_\rho </math> refers to the structure tensor related with the image pixel point (i,j) having standard deviation <math>\rho</math>. <math>G</math> refers to the Gaussian kernel <math>(0, \rho ^2)</math> with standard deviation <math>\rho</math>. <math>\sigma</math> refers to the manually defined parameter for the image <math>I</math> below which the edge detection is insensitive to noise. <math>\nabla I_{\sigma}</math> refers to the gradient of the image <math>I</math> and <math>(\nabla I_{\sigma} \otimes \nabla I_{\sigma})</math> refers to the tensor product obtained by using this gradient.<ref name="Orientation and directional refinement">{{Cite journal | doi=10.1109/LSP.2013.2280571| title=Iterative Directional Total Variation Refinement for Compressive Sensing Image Reconstruction| journal=IEEE Signal Processing Letters| volume=20| issue=11| pages=1070–1073| year=2013| last1=Xuan Fei| last2=Zhihui Wei| last3=Liang Xiao|bibcode = 2013ISPL...20.1070F| s2cid=8156085}}</ref>

The structure tensor obtained is convolved with a Gaussian kernel <math>G</math> to improve the accuracy of the orientation estimate with <math>\sigma</math> being set to high values to account for the unknown noise levels. For every pixel (i,j) in the image, the structure tensor J is a symmetric and positive semi-definite matrix. Convolving all the pixels in the image with <math>G</math>, gives orthonormal eigen vectors  ω and υ of the <math>J</math> matrix. ω points in the direction of the dominant orientation having the largest contrast and υ points in the direction of the structure orientation having the smallest contrast.  The orientation field coarse initial estimation <math>\hat{d}</math> is defined as <math>\hat{d}</math> = υ.  This estimate is accurate at strong edges. However, at weak edges or on regions with noise, its reliability decreases.

To overcome this drawback, a refined orientation model is defined in which the data term reduces the effect of noise and improves accuracy while the second penalty term with the L2-norm is a fidelity term which ensures accuracy of initial coarse estimation.

This orientation field is introduced into the directional total variation optimization model for CS reconstruction through the equation: <math>min_\Chi\lVert \nabla \Chi \bullet d \rVert _{1} + \frac{\lambda}{2}\ \lVert Y - \Phi\Chi \rVert ^2_{2}</math>. <math>\Chi</math> is the objective signal which needs to be recovered. Y is the corresponding measurement vector, d is the iterative refined orientation field and <math>\Phi</math> is the CS measurement matrix. This method undergoes a few iterations ultimately leading to convergence.<math>\hat{d}</math> is the orientation field approximate estimation of the reconstructed image <math>X^{k-1}</math> from the previous iteration (in order to check for convergence and the subsequent optical performance, the previous iteration is used). For the two vector fields represented by  <math>\Chi</math> and <math>d</math>, <math>\Chi \bullet d</math> refers to the multiplication of respective horizontal and vertical vector elements of <math>\Chi</math> and <math>d</math> followed by their subsequent addition. These equations are reduced to a series of convex minimization problems which are then solved with a combination of variable splitting and augmented Lagrangian (FFT-based fast solver with a closed form solution) methods.<ref name = "Orientation and directional refinement" /> It (Augmented Lagrangian) is considered equivalent to the split Bregman iteration which ensures convergence of this method.  The orientation field, d is defined as being equal to <math>(d_{h}, d_{v})</math>, where <math>d_{h},  d_{v}</math> define the horizontal and vertical estimates of <math>d</math>.

[[File:Augmented Lagrangian.png|thumb|right|Augmented Lagrangian method for orientation field and iterative directional field refinement models]]

The Augmented Lagrangian method for the orientation field,  <math>min_\Chi\lVert \nabla \Chi \bullet d \rVert _{1} + \frac{\lambda}{2}\ \lVert Y - \Phi\Chi \rVert ^2_{2}</math>,  involves initializing <math>d_{h}, d_{v}, H, V</math> and then finding the approximate minimizer of <math>L_{1}</math> with respect to these variables.  The Lagrangian multipliers are then updated and the iterative process is stopped when convergence is achieved. For the iterative directional total variation refinement model, the augmented lagrangian method involves initializing <math>\Chi, P, Q, \lambda_{P}, \lambda_{Q}</math>.<ref name="TV">{{cite journal | last1 = Goldluecke | first1 = B. | last2 = Strekalovskiy | first2 = E. | last3 = Cremers | first3 = D. | last4 = Siims | first4 = P.-T. A. I. | year = 2012 | title = The natural vectorial total variation which arises from geometric measure theory | journal = SIAM J. Imaging Sci. | volume = 5 | issue = 2| pages = 537–563 | doi=10.1137/110823766| citeseerx = 10.1.1.364.3997 }}</ref>

Here, <math>H, V, P, Q</math> are newly introduced variables where <math>H</math> = <math>\nabla d_{h}</math>, <math>V</math> = <math>\nabla d_{v}</math>, <math>P</math> = <math>\nabla \Chi</math>, and <math>Q</math> = <math>P \bullet d</math>. <math>\lambda_{H}, \lambda_{V}, \lambda_{P}, \lambda_{Q}</math> are the Lagrangian multipliers for <math>H, V, P, Q</math>. For each iteration, the approximate minimizer of <math>L_{2}</math> with respect to variables (<math>\Chi, P, Q</math>) is calculated. And as in the field refinement model, the lagrangian multipliers are updated and the iterative process is stopped when convergence is achieved.

For the orientation field refinement model, the Lagrangian multipliers are updated in the iterative process as follows:

<math>(\lambda_{H})^k = (\lambda_{H})^{k-1} +  \gamma_{H}(H^k - \nabla (d_{h})^k)</math>

<math>(\lambda_{V})^k = (\lambda_{V})^{k-1} +  \gamma_{V}(V^k - \nabla (d_{v})^k)</math>

For the iterative directional total variation refinement model, the Lagrangian multipliers are updated as follows:

<math>(\lambda_{P})^k = (\lambda_{P})^{k-1} +  \gamma_{P}(P^k - \nabla (\Chi)^k)</math>

<math>(\lambda_{Q})^k = (\lambda_{Q})^{k-1} +  \gamma_{Q}(Q^k - P^{k} \bullet d)</math>

Here, <math>\gamma_{H}, \gamma_{V}, \gamma_{P}, \gamma_{Q}</math> are positive constants.

=====Advantages and disadvantages=====

Based on [[peak signal-to-noise ratio]] (PSNR) and [[structural similarity]] index (SSIM) metrics and known ground-truth images for testing performance, it is concluded that iterative directional total variation has a better reconstructed performance than the non-iterative methods in preserving edge and texture areas. The orientation field refinement model plays a major role in this improvement in performance as it increases the number of directionless pixels in the flat area while enhancing the orientation field consistency in the regions with edges.

==Applications==
The field of compressive sensing is related to several topics in signal processing and computational mathematics, such as [[underdetermined system|underdetermined linear-system]]s, [[group testing]], heavy hitters, [[sparse coding]], [[multiplexing]], sparse sampling, and finite rate of innovation. Its broad scope and generality has enabled several innovative CS-enhanced approaches in signal processing and compression, solution of inverse problems, design of radiating systems, radar and through-the-wall imaging, and antenna characterization.<ref>{{Cite journal|author1=Andrea Massa |author2=Paolo Rocca |author3=Giacomo Oliveri |title = Compressive Sensing in Electromagnetics – A Review|journal = IEEE Antennas and Propagation Magazine|volume = 57|pages=224–238 |number = 1|year = 2015|doi = 10.1109/MAP.2015.2397092|bibcode=2015IAPM...57..224M|s2cid=30196057 }}</ref>  Imaging techniques having a strong affinity with compressive sensing include [[coded aperture]] and [[computational photography]].

Conventional CS reconstruction uses sparse signals (usually sampled at a rate less than the Nyquist sampling rate) for reconstruction through constrained <math>l_{1}</math> minimization. One of the earliest applications of such an approach was in reflection seismology which used sparse reflected signals from band-limited data for tracking changes between sub-surface layers.<ref name="Seismic sparse signals">{{cite journal | last1 = Taylor | first1 = H.L. | last2 = Banks | first2 = S.C. | last3 = McCoy | first3 = J.F. | year = 1979 | title = Deconvolution with the 1 norm | journal = Geophysics | volume = 44 | issue = 1| pages = 39–52 | doi = 10.1190/1.1440921 }}</ref>  When the LASSO model came into prominence in the 1990s as a statistical method for selection of sparse models,<ref name="LASSO">{{cite journal | last1 = Tibshirani | first1 = R | year = 1996 | title = Regression shrinkage and selection via the lasso | url = http://www-stat.stanford.edu/~tibs/lasso/lasso.pdf | journal = J. R. Stat. Soc. B | volume = 58 | issue = 1| pages = 267–288 | doi = 10.1111/j.2517-6161.1996.tb02080.x }}</ref> this method was further used in computational harmonic analysis for sparse signal representation from over-complete dictionaries. Some of the other applications include incoherent sampling of radar pulses. The work by ''Boyd et al.''<ref name = "Original source for IRLS" /> has applied the LASSO model- for selection of sparse models- towards analog to digital converters (the current ones use a sampling rate higher than the Nyquist rate along with the quantized Shannon representation). This would involve a parallel architecture in which the polarity of the analog signal changes at a high rate followed by digitizing the integral at the end of each time-interval to obtain the converted digital signal.

===Photography===
Compressed sensing is used in a mobile phone camera sensor. The approach allows a reduction in image acquisition energy per image by as much as a factor of 15 at the cost of complex decompression algorithms; the computation may require an off-device implementation.<ref>{{cite magazine|title=New Camera Chip Captures Only What It Needs|author=David Schneider|journal=IEEE Spectrum|date=March 2013|url=https://spectrum.ieee.org/semiconductors/optoelectronics/camera-chip-makes-alreadycompressed-images|access-date=2013-03-20}}</ref>

Compressed sensing is used in single-pixel cameras from [[Rice University]].<ref name=cscamera>{{cite web |url=http://dsp.rice.edu/cscamera |archive-url=https://web.archive.org/web/20100605170550/http://dsp.rice.edu/cscamera |url-status=dead |archive-date=2010-06-05 |title=Compressive Imaging: A New Single-Pixel Camera |website=Rice DSP |access-date=2013-06-04 }}</ref> [[Bell Labs]] employed the technique in a lensless single-pixel camera that takes stills using repeated snapshots of randomly chosen apertures from a grid. Image quality improves with the number of snapshots, and generally requires a small fraction of the data of conventional imaging, while eliminating lens/focus-related aberrations.<ref>{{cite web |url=http://www.technologyreview.com/view/515651/bell-labs-invents-lensless-camera/ |title=Bell Labs Invents Lensless Camera |website=MIT Technology Review |date=2013-05-25 |access-date=2013-06-04}}</ref><ref>{{cite conference|author1=Gang Huang|author2=Hong Jiang|author3=Kim Matthews|author4=Paul Wilford|title=Lensless Imaging by Compressive Sensing|year=2013|volume=2393|pages=2101–2105|conference=2013 IEEE International Conference on Image Processing |arxiv=1305.7181 |bibcode=2013arXiv1305.7181H |doi=10.1109/ICIP.2013.6738433 |isbn=978-1-4799-2341-0}}</ref>

===Holography===
Compressed sensing can be used to improve image reconstruction in [[holography]] by increasing the number of [[voxel]]s one can infer from a single hologram.<ref>{{cite journal | last1 = Brady | first1 = David | last2 = Choi | first2 = Kerkil | last3 = Marks | first3 = Daniel | last4 = Horisaki | first4 = Ryoichi | last5 = Lim | first5 = Sehoon | year = 2009 | title = Compressive holography | journal = Optics Express | volume = 17 | issue = 15| pages = 13040–13049 | doi=10.1364/oe.17.013040| pmid = 19654708 | bibcode = 2009OExpr..1713040B }}</ref><ref>{{cite journal | last1 = Rivenson | first1 = Y. | last2 = Stern | first2 = A. | last3 = Javidi | first3 = B. | year = 2010 | title = Compressive fresnel holography | journal = Display Technology, Journal of | volume = 6 | issue = 10| pages = 506–509 | doi=10.1109/jdt.2010.2042276| bibcode = 2010JDisT...6..506R | citeseerx = 10.1.1.391.2020 | s2cid = 7460759 }}</ref><ref>{{cite journal | last1 = Denis | first1 = Loic | last2 = Lorenz | first2 = Dirk | last3 = Thibaut | first3 = Eric | last4 = Fournier | first4 = Corinne | last5 = Trede | first5 = Dennis | year = 2009 | title = Inline hologram reconstruction with sparsity constraints | url = https://hal-ujm.archives-ouvertes.fr/ujm-00397994/file/OL_sparse_reconstruction_DH_revised_2line.pdf| journal = Opt. Lett. | volume = 34 | issue = 22| pages = 3475–3477 | doi=10.1364/ol.34.003475| pmid = 19927182 | bibcode = 2009OptL...34.3475D }}</ref> It is also used for image retrieval from undersampled measurements in optical<ref>{{cite journal | last1 = Marim | first1 = M. | last2 = Angelini | first2 = E. | last3 = Olivo-Marin | first3 = J. C. | last4 = Atlan | first4 = M. | year = 2011 | title = Off-axis compressed holographic microscopy in low-light conditions | arxiv = 1101.1735| journal = Optics Letters | volume = 36 | issue = 1| pages = 79–81 | doi=10.1364/ol.36.000079| pmid = 21209693 | bibcode = 2011OptL...36...79M | s2cid = 24074045 }}</ref><ref>{{cite journal | last1 = Marim | first1 = M. M. | last2 = Atlan | first2 = M. | last3 = Angelini | first3 = E. | last4 = Olivo-Marin | first4 = J. C. | year = 2010 | title = Compressed sensing with off-axis frequency-shifting holography | arxiv = 1004.5305| journal = Optics Letters | volume = 35 | issue = 6| pages = 871–873 | doi=10.1364/ol.35.000871| pmid = 20237627 | bibcode = 2010OptL...35..871M | s2cid = 9738556 }}</ref> and millimeter-wave<ref>{{cite journal | last1 = Fernandez Cull | first1 = Christy | last2 = Wikner | first2 = David A. | last3 = Mait | first3 = Joseph N. | last4 = Mattheiss | first4 = Michael | last5 = Brady | first5 = David J. | year = 2010 | title = Millimeter-wave compressive holography | journal = Appl. Opt. | volume = 49 | issue = 19| pages = E67–E82 | doi=10.1364/ao.49.000e67| pmid = 20648123 | bibcode = 2010ApOpt..49E..67C | citeseerx = 10.1.1.1018.5231 }}</ref> holography.

===Facial recognition===
Compressed sensing is being used in facial recognition applications.<ref>[https://www.wired.com/science/discoveries/news/2008/03/new_face_recognition Engineers Test Highly Accurate Face Recognition]</ref>

===Magnetic resonance imaging===
Compressed sensing has been used<ref name="dx.doi.org">{{cite journal | doi = 10.1002/mrm.21391 | volume=58 | title=Sparse MRI: The application of compressed sensing for rapid MR imaging | year=2007 | journal=Magnetic Resonance in Medicine | pages=1182–1195 | last1 = Lustig | first1 = Michael| issue=6 | pmid=17969013 | s2cid=15370510 }}</ref><ref name="Compressed Sensing MRI 2008">{{cite journal | last1 = Lustig | first1 = M. | last2 = Donoho | first2 = D.L. | last3 = Santos | first3 = J.M. | last4 = Pauly | first4 = J.M. | year = 2008 | title = Compressed Sensing MRI; | journal = IEEE Signal Processing Magazine| volume = 25 | issue = 2| pages = 72–82 | doi = 10.1109/MSP.2007.914728 | bibcode = 2008ISPM...25...72L | s2cid = 945906 }}</ref>  to shorten [[magnetic resonance imaging]] scanning sessions on conventional hardware.<ref>{{cite journal|author=Jordan EllenbergEmail Author |url=https://www.wired.com/magazine/2010/02/ff_algorithm/all/1 |title=Fill in the Blanks: Using Math to Turn Lo-Res Datasets Into Hi-Res Samples &#124; Wired Magazine |journal=Wired |volume=18 |issue=3 |date=2010-03-04 |access-date=2013-06-04}}</ref><ref>[http://nuit-blanche.blogspot.com/2010/03/why-compressed-sensing-is-not-csi.html Why Compressed Sensing is NOT a CSI "Enhance" technology ... yet !]</ref><ref>[http://nuit-blanche.blogspot.com/2010/03/surely-you-must-be-joking-mr.html Surely You Must Be Joking Mr. Screenwriter]</ref> Reconstruction methods include
* ISTA
* FISTA
* SISTA
* ePRESS<ref>{{cite journal|last1=Zhang|first1=Y.|last2=Peterson|first2=B.|title=Energy Preserved Sampling for Compressed Sensing MRI|journal=Computational and Mathematical Methods in Medicine|date=2014|volume=2014|doi=10.1155/2014/546814|pmid=24971155|pmc=4058219|pages=546814|bibcode=2015CMMM.201514104T|arxiv=1501.03915}}</ref>
* EWISTA<ref name=Zhang_2015>{{cite journal|last1=Zhang|first1=Y.|title=Exponential Wavelet Iterative Shrinkage Thresholding Algorithm for Compressed Sensing Magnetic Resonance Imaging|journal=Information Sciences|date=2015|volume=322|pages=115–132|doi=10.1016/j.ins.2015.06.017}}</ref>
* EWISTARS<ref>{{cite journal|last1=Zhang|first1=Y.|last2=Wang|first2=S.|title=Exponential Wavelet Iterative Shrinkage Thresholding Algorithm with Random Shift for Compressed Sensing Magnetic Resonance Imaging|journal=IEEJ Transactions on Electrical and Electronic Engineering|date=2015|volume=10|issue=1|pages=116–117|doi=10.1002/tee.22059}}</ref> etc.

Compressed sensing addresses the issue of high scan time by enabling faster acquisition by measuring fewer Fourier coefficients. This produces a high-quality image with relatively lower scan time. Another application (also discussed ahead) is for CT reconstruction with fewer X-ray projections. Compressed sensing, in this case, removes the high spatial gradient parts – mainly, image noise and artifacts. This holds tremendous potential as one can obtain high-resolution CT images at low radiation doses (through lower current-mA settings).<ref name="MRI">{{cite journal | last1 = Figueiredo | first1 = M. | last2 = Bioucas-Dias | first2 = J.M. | last3 = Nowak | first3 = R.D. | year = 2007 | title = Majorization–minimization algorithms for wavelet-based image restoration | url = https://zenodo.org/record/896391| journal = IEEE Trans. Image Process. | volume = 16 | issue = 12| pages = 2980–2991 | doi=10.1109/tip.2007.909318| pmid = 18092597 | bibcode = 2007ITIP...16.2980F | s2cid = 8160052 }}</ref>

===Network tomography===
Compressed sensing has showed outstanding results in the application of [[network tomography]] to [[network management]]. [[Network delay]] estimation and [[network congestion]] detection can both be modeled as underdetermined [[System of linear equations|systems of linear equations]] where the coefficient matrix is the network routing matrix. Moreover, in the [[Internet]], network routing matrices usually satisfy the criterion for using compressed sensing.<ref>[Network tomography via compressed sensing|http://www.ee.washington.edu/research/funlab/Publications/2010/CS-Tomo.pdf]</ref>

===Shortwave-infrared cameras===
Commercial shortwave-infrared cameras based upon compressed sensing are available.<ref>{{cite web|title=InView web site|url=http://www.inviewcorp.com/products |website=inviewcorp.com}}</ref> These cameras have light sensitivity from 0.9&nbsp;[[µm]] to 1.7&nbsp;µm, which are wavelengths invisible to the human eye.

===Aperture synthesis in radio astronomy===
In the field of [[radio astronomy]], compressed sensing has been proposed for deconvolving an interferometric image.<ref>[https://web.archive.org/web/20160916125614/http://mnras.oxfordjournals.org/content/395/3/1733 |Compressed sensing imaging techniques for radio interferometry]</ref> In fact, the [[CLEAN (algorithm)|Högbom CLEAN algorithm]] that has been in use for the deconvolution of radio images since 1974, is similar to compressed sensing's matching pursuit algorithm.

===Transmission electron microscopy===
Compressed sensing combined with a moving aperture has been used to increase the acquisition rate of images in a [[transmission electron microscopy|transmission electron microscope]].<ref>{{cite journal|last1=Stevens|first1=Andrew|last2=Kovarik|first2=Libor|last3=Abellan|first3=Patricia|last4=Yuan|first4=Xin|last5=Carin|first5=Lawrence|last6=Browning|first6=Nigel D.|title=Applying compressive sensing to TEM video: a substantial frame rate increase on any camera|journal=Advanced Structural and Chemical Imaging|date=13 August 2015|volume=1|issue=1|doi=10.1186/s40679-015-0009-3|doi-access=free}}</ref> In [[Scanning transmission electron microscopy|scanning mode]], compressive sensing combined with random scanning of the electron beam has enabled both faster acquisition and less electron dose, which allows for imaging of electron beam sensitive materials.<ref>{{cite journal|last1=Kovarik|first1=L.|last2=Stevens|first2=A.|last3=Liyu|first3=A.|last4=Browning|first4=N. D.|title=Implementing an accurate and rapid sparse sampling approach for low-dose atomic resolution STEM imaging|journal=Applied Physics Letters|date=17 October 2016|volume=109|issue=16|pages=164102|doi=10.1063/1.4965720|bibcode=2016ApPhL.109p4102K|url=https://zenodo.org/record/1232079}}</ref>

==See also==
*[[Noiselet]]
*[[Sparse approximation]]
*[[Sparse coding]]
*[[Low-density parity-check code]]
*[[Compressed sensing in speech signals]]

==Notes==
{{Reflist|group=note}}

==References==
{{Reflist|30em}}

==Further reading==
* "The Fundamentals of Compressive Sensing" [http://www.brainshark.com/brainshark/brainshark.net/portal/title.aspx?pid=zCdz10BfTRz0z0 Part 1], [http://www.brainshark.com/brainshark/brainshark.net/portal/title.aspx?pid=zCgzXgcEKz0z0 Part 2] and [http://www.brainshark.com/brainshark/brainshark.net/portal/title.aspx?pid=zAvz9F41cz0z0 Part 3]: video tutorial by Mark Davenport, Georgia Tech. at [http://www.brainshark.com/sps SigView, the IEEE Signal Processing Society Tutorial Library].
* [https://www.wired.com/magazine/2010/02/ff_algorithm/all/1 Using Math to Turn Lo-Res Datasets Into Hi-Res Samples] Wired Magazine article
* [http://arquivo.pt/wayback/20160516193158/http://dsp.rice.edu/cs/ Compressive Sensing Resources] at [[Rice University]].
* [http://www.ams.org/happening-series/hap7-pixel.pdf Compressed Sensing Makes Every Pixel Count] – article in the AMS ''What's Happening in the Mathematical Sciences'' series
* [https://web.archive.org/web/20150504060355/http://ugcs.caltech.edu/~srbecker/wiki/Main_Page Wiki on sparse reconstruction]

{{DEFAULTSORT:Compressed Sensing}}
[[Category:Information theory]]
[[Category:Signal estimation]]
[[Category:Linear algebra]]
[[Category:Mathematical optimization]]